# Databricks notebook source
# MAGIC %md
# MAGIC
# MAGIC # Curate Articles
# MAGIC
# MAGIC This notebook will run first a first level parsing and save article content into a delta table. This step of saving files to an intermediate table before further processing is desired because it consolidates many articles into a single file and removes the first level of the element tree. Both improve performance for iterative use.
# MAGIC
# MAGIC This task will append new articles in `biomed.curated_articles_xml` table. If a document will be updated, a new AccessionID/PMID will be generated by PMC and the old one will be retracted.
# MAGIC
# MAGIC **TODO**: Add redactions 

# COMMAND ----------

# MAGIC %run ./config/setup_workflow $SHOW_TABLE=false $SHOW_GRAPHIC=true

# COMMAND ----------

# curated_articles_xml have inner XML for fields front, body, floatgroups, back, and processing_metadata

sql_path = biomed.curated_articles_xml.sql_path
with open(sql_path, 'r') as file:
    sql = file.read()
    print(sql)

# COMMAND ----------

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, IntegerType, DoubleType, MapType, StructType, StructField

# Define the UDF
def article_parse_xml(accession_id: str, volume_path: str) -> tuple:
    from bs4 import BeautifulSoup
    with open(volume_path, 'r') as file:
        article = BeautifulSoup(file.read(), 'xml').find('article')
    return (accession_id, volume_path,
        {str(k):str(v) for k,v in article.attrs.items()},
        str(article.find('front')),
        str(article.find('body')),
        str(article.find('floats-group')),
        str(article.find('back')),
        # TODO: update processing metadata type to map
        str(article.find('processing-meta')))

# Register the UDF
article_parse_xml_udf = udf(article_parse_xml, 
                            returnType=StructType([StructField("AccessionID",  StringType(), nullable=False),
                                                   StructField("volume_path",  StringType(), nullable=False),
                                                   StructField("attrs",        MapType(StringType(), StringType()), nullable=False),
                                                   StructField("front",         StringType(), nullable=False),
                                                   StructField("body",         StringType(), nullable=False),
                                                   StructField("floats_group", StringType(), nullable=False),
                                                   StructField("back", StringType(), nullable=False),
                                                   StructField("processing_metadata", StringType(), nullable=False)]))

# COMMAND ----------

parsed_articles = biomed.raw_metadata_xml.df.filter('status="DOWNLOADED"') \
                        .join(biomed.curated_articles_xml.df, 'AccessionID', 'leftanti') \
                        .withColumn('parsed_struct', article_parse_xml_udf("AccessionID", "volume_path")) \
                        .select('parsed_struct.AccessionID',
                                'ETag',
                                'LastUpdated',
                                'PMID',
                                'parsed_struct.attrs',
                                'parsed_struct.front',
                                'parsed_struct.body',
                                'parsed_struct.floats_group',
                                'parsed_struct.back',
                                'parsed_struct.processing_metadata',
                                '_ingestion_timestamp',
                                'volume_path')

# COMMAND ----------

merged_parsed_articles = biomed.curated_articles_xml.dt.alias('tgt') \
                               .merge(parsed_articles.alias('src'), "src.AccessionID = tgt.AccessionID") \
                               .whenMatchedUpdateAll() \
                               .whenNotMatchedInsertAll() \
                               .execute()
